{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7393f-b072-4bf7-8323-8aad86f1cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab1ccab-c011-4c70-ae85-4f6fad9a3533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as sp\n",
    "\n",
    "sc = sp.SparkContext.getOrCreate()\n",
    "print(sc)\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7436a4cc-4129-4e8a-90ee-9947ab6da84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import SparkSeccion pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create my_spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#print my_spark\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb22f21-a5af-4625-bca1-b22d2db94db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create pd_temp\n",
    "pd_temp = pd.DataFrame(np.random.random(10))\n",
    "\n",
    "# Create spark_temp from pd_temp\n",
    "spark_temp = spark.createDataFrame(pd_temp)\n",
    "\n",
    "# Examine the tables in the catalog\n",
    "print(spark.catalog.listTables())\n",
    "\n",
    "# Add spark_temp to the catalog\n",
    "spark_temp.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "# Examine the tables in the catalog again\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e9a03-ce92-4fb2-9396-b58b918316ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Downloads/Airpots Project/airports.csv'\n",
    "\n",
    "#Read in the airports path\n",
    "airports = spark.read.csv(file_path, header=True)\n",
    "\n",
    "airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ade5df-4572-49eb-950e-38fad289f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(airports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d0606-8a47-413a-8283-c85a3b1a768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11618c89-306b-41f7-8867-00d11511b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16856fe7-4618-44f7-84dd-1ff54423e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = spark.read.csv('Downloads/Airpots Project/flights_small.csv', header=True)\n",
    "flights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab30109-98dd-467c-b14d-9e118768fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.name = flights.createOrReplaceTempView('flights')\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec4c5d-41e8-4585-bbc2-408ec6957529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame flights\n",
    "flights_df = spark.table('flights')\n",
    "print(flights_df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3ba1fd-0249-49c7-b790-b4df18df15ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include a new column called duration_hrs\n",
    "flights = flights.withColumn('duration_hrs', flights.air_time / 60)\n",
    "flights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b8a73-eca4-4482-9b87-e3180fe9658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bb5f95-a394-4c91-baa0-29a203d326c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter flights with a SQL string\n",
    "long_flights1 = flights.filter('distance > 1000')\n",
    "long_flights1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ee39d-17f7-44d2-bf58-524d3fed0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter flights with a boolean column\n",
    "long_flights2 = flights.filter(flights.distance > 1000 )\n",
    "long_flights2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115e5b9-2c9a-496d-954d-2a7708a17351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first set of columns as a string\n",
    "selected_1 = flights.select('tailnum', 'origin', 'dest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd9d67-a483-4c7a-b26c-45341b5eadfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the second set of columns usinf df.col_name\n",
    "temp = flights.select(flights.origin, flights.dest, flights.carrier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b3f451-8403-4e5a-a193-e29fd10a4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define first filter to only keep flights from SEA to PDX.\n",
    "FilterA = flights.origin == 'SEA'\n",
    "FilterB =flights.dest == 'PDX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b1ce7f-8a44-4d74-8fdc-f68978e71621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data, first by filterA then by filterB\n",
    "selected_2 = temp.filter(FilterA).filter(FilterB)\n",
    "selected_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c04d628-bf6e-4cbd-a1f0-52017578cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a table of the average speed of each flight both ways.\n",
    "#Calculate average speed by dividing the distance by the air_time (converted to hours).Use the .alias() method name\n",
    "# Define avg_speed\n",
    "avg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n",
    "speed_1 = flights.select('origin','dest','tailnum', avg_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b72c1d-db42-4f4c-972f-d8b31015a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the Spark DataFrame method .selectExpr() \n",
    "speed_2 =flights.selectExpr('origin','dest','tailnum','distance/(air_time/60) as avg_speed')\n",
    "speed_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca6f68-ce99-433a-845a-f328ecaa7c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124152d0-9c42-4372-9781-8dcf013acd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr_time: string and distance: string, so to find min() and max() we need to convert this float \n",
    "flights = flights.withColumn('distance', flights.distance.cast('float'))\n",
    "flights = flights.withColumn('air_time', flights.air_time.cast('float'))\n",
    "\n",
    "flights.describe('air_time', 'distance').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df92c6cf-979f-4022-822b-9b23a8ddf2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the length of the shortest (in terms of distance) flight that left PDX \n",
    "flights.filter(flights.origin =='PDX').groupBy().min('distance').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf23a43-f194-424a-b3a6-9eea2cb2f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the length of the longest (in terms of time) flight that left SEA\n",
    "flights.filter(flights.origin == 'SEA').groupBy().max('air_time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a7f3f-3dd9-45a8-92d0-508349927105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the average air time of Delta Airlines flights  that left SEA. \n",
    "flights.filter(flights.carrier == 'DL').filter(flights.origin == 'SEA').groupBy().avg('air_time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a21a3-b24f-4dce-afc5-4777bf3eb9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the total number of hours all planes in this dataset spent in the air by creating a column called duration_hrs\n",
    "flights.withColumn('duration_hrs', flights.air_time/60).groupBy().sum('duration_hrs').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d15063-402d-4ed4-a355-e00018f741b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by tailnum column\n",
    "by_plane = flights.groupBy('tailnum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f43558-3e1f-4f13-a7a3-85ecd2d4fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the .count() method with no arguments to count the number of flights each plane made\n",
    "by_plane.count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3768d73-14de-49a0-b4ae-33889fac9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by origin column\n",
    "by_origin = flights.groupBy('origin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9179bd5-ca73-4381-8e16-1d0c8175e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the .avg() of the air_time column to find average duration of flights from PDX and SEA\n",
    "by_origin.avg('air_time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeeb90c-355a-45b9-88f7-962e8947a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyspark.sql.functions as F\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "#convert to dep_delay to numeric column\n",
    "flights = flights.withColumn('dep_delay', flights.dep_delay.cast('float'))\n",
    "\n",
    "# Group by month and dest\n",
    "by_month_dest = flights.groupBy('month', 'dest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fecf348-490a-447f-a6b1-04e4a8cfff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average departure delay by month and destination\n",
    "by_month_dest.avg('dep_delay').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8502a-dba1-4d75-897b-762954d7041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e8909-7715-4f45-99da-1c45a220e109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the faa column\n",
    "airports = airports.withColumnRenamed('faa','dest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31bd36-16e2-45e8-bd78-d35e8fde7704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the DataFrames\n",
    "flights_with_airports= flights.join(airports, on='dest', how='leftouter')\n",
    "flights_with_airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c41e9ea-463c-4094-9eaf-ce7469be761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "planes = spark.read.csv('Downloads/Airpots Project/planes.csv', header=True)\n",
    "planes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa0e821-0cc1-463b-a732-2ed85774f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename year column on panes to avoid duplicate column name\n",
    "planes = planes.withColumnRenamed('year', 'plane_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a17ffb-cc72-48f7-828a-eb94825f3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the flights and plane table use key as tailnum column\n",
    "model_data = flights.join(planes, on='tailnum', how='leftouter')\n",
    "model_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963f1605-7a26-4108-add9-83aa7293ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a57de4b-af99-42c6-8683-bc2ab32e7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data.withColumn('arr_delay', model_data.arr_delay.cast('integer'))\n",
    "model_data = model_data.withColumn('air_time' , model_data.air_time.cast('integer'))\n",
    "model_data = model_data.withColumn('month', model_data.month.cast('integer'))\n",
    "model_data = model_data.withColumn('plane_year', model_data.plane_year.cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791481c9-0e83-4472-b414-be6dbe9af5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.describe('arr_delay', 'air_time','month', 'plane_year').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c1aa28-7822-4b9a-9c40-79c6878fd916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column\n",
    "model_data =model_data.withColumn('plane_age', model_data.year - model_data.plane_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b810e0e-a053-4a02-b649-bf31e7d72a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data.withColumn('is_late', model_data.arr_delay >0)\n",
    "\n",
    "model_data = model_data.withColumn('label', model_data.is_late.cast('integer'))\n",
    "\n",
    "model_data.filter(\"arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fb01f7-1204-4024-a848-75d31201fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0288d3bb-8b6c-4a6b-8125-104817c42e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a StringIndexer\n",
    "carr_indexer = StringIndexer(inputCol='carrier', outputCol='carrier_index')\n",
    "#Create a OneHotEncoder\n",
    "carr_encoder = OneHotEncoder(inputCol='carrier_index', outputCol='carr_fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ecc7dc-1594-4198-b52e-eb7d331b6336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the dest column just like you did above\n",
    "dest_indexer = StringIndexer(inputCol='dest', outputCol='dest_index')\n",
    "dest_encoder = OneHotEncoder(inputCol='dest_index', outputCol='dest_fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617e8bb-a837-49bc-a2a8-3ddf82b7156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble a  Vector\n",
    "from pyspark.ml.feature import  VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a3e5e1-87fb-4cf0-bd9f-fffc0089a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler =VectorAssembler(inputCols=['month', 'air_time','carr_fact','dest_fact','plane_age'],\n",
    "                              outputCol='features',handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a617788d-5b09-423b-b9da-325db3b846f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Create the pipeline\n",
    "# You're finally ready to create a` Pipeline!` Pipeline is a class in the `pyspark.ml module` that combines all the Estimators and Transformers that you've already created.\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b755de0-440d-45c0-b27f-1293105136d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "piped_data =flights_pipe.fit(model_data).transform(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde9c90c-f676-4c37-96d1-464be79a5bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "piped_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4305ad5-b97f-4875-bdd7-53c0a12007c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = piped_data.randomSplit([.6, .4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3d91e-13c7-4dc5-8217-daeb82cdf34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345fc980-4b56-4577-a554-fc6611ed3507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Create the evaluator\n",
    "# The first thing you need when doing cross validation for model selection is a way to compare different models. Luckily, the pyspark.ml.evaluation submodule has classes for evaluating different kinds of models. Your model is a binary classification model, so you'll be using the `BinaryClassificationEvaluator` from the `pyspark.ml.evaluation` module. This evaluator calculates the area under the ROC. This is a metric that combines the two kinds of errors a binary classifier can make (false positives and false negatives) into a simple number.\n",
    "\n",
    "import pyspark.ml.evaluation as evals\n",
    "\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4625e-71bc-416b-8bd3-6666f61d43fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tuning submodule\n",
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "# Create the parameter grid\n",
    "grid = tune.ParamGridBuilder()\n",
    "\n",
    "# Add the hyperparameter\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0,1])\n",
    "\n",
    "# Build the grid\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a76369-52a0-4435-88fd-0ef2d512e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CrossValidator\n",
    "cv = tune.CrossValidator(estimator=lr,\n",
    "               estimatorParamMaps=grid,\n",
    "               evaluator=evaluator\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2811cd41-fc1f-44a8-b606-4dceaaf407e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit cross validation models\n",
    "models = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18197fb-715d-42bb-b48f-5275bb909615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best model\n",
    "best_lr = models.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7f846-e733-4e41-9b1b-63cdbd8ccdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict the test set\n",
    "test_results = best_lr.transform(test)\n",
    "# \n",
    "# Evaluate the predictions\n",
    "print(evaluator.evaluate(test_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
